llama-cpp-python[server]>=0.2.0
mcp>=0.9.0
python-dotenv>=1.0.0
requests>=2.31.0
pyyaml>=6.0
asyncio>=3.4.3
uvicorn>=0.27.0
httpx>=0.26.0
